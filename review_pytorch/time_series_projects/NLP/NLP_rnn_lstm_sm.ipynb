{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c296ab7e-438c-4cf9-9a64-80ca83ec1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebaf4115-1ad0-4b77-92c2-b74d54c87ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2750340-d49b-470f-aa9e-69eba83851c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/shakespeare.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96eb590-36a9-4452-aecf-9bf37e524740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e24da3-d80d-4a59-a0a8-27ba18e5cdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3a75d81-a46c-4767-ac4c-654bcf3a79e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaf4d8c3-64c9-4136-bbe3-21f4ae666bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l,\\n  Making a famine where abundance lies,\\n  Thy s'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[300:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ee86eb-0597-497c-8135-3d0d3a5af232",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a88ca1d1-bc91-4c7a-85ac-5b02438e837d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " '}'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "901e770c-1bba-4991-ae36-2fd02f6780bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "669f30e8-9273-4a6c-a7be-63ef163539dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ')')\n",
      "(1, '8')\n",
      "(2, 'J')\n",
      "(3, 'V')\n",
      "(4, '}')\n",
      "(5, 'X')\n",
      "(6, 't')\n",
      "(7, '4')\n",
      "(8, 'h')\n",
      "(9, 'x')\n",
      "(10, 'b')\n",
      "(11, 'I')\n",
      "(12, 'Y')\n",
      "(13, '9')\n",
      "(14, '`')\n",
      "(15, 'e')\n",
      "(16, '-')\n",
      "(17, 'E')\n",
      "(18, '[')\n",
      "(19, 'c')\n",
      "(20, ']')\n",
      "(21, 'a')\n",
      "(22, '!')\n",
      "(23, 'S')\n",
      "(24, 'D')\n",
      "(25, '\\n')\n",
      "(26, 'B')\n",
      "(27, 'H')\n",
      "(28, '\"')\n",
      "(29, '|')\n",
      "(30, ';')\n",
      "(31, '5')\n",
      "(32, 'g')\n",
      "(33, 'm')\n",
      "(34, 'U')\n",
      "(35, 'W')\n",
      "(36, '2')\n",
      "(37, '3')\n",
      "(38, 'j')\n",
      "(39, '_')\n",
      "(40, '6')\n",
      "(41, 'n')\n",
      "(42, 's')\n",
      "(43, '0')\n",
      "(44, 'p')\n",
      "(45, 'q')\n",
      "(46, 'f')\n",
      "(47, 'C')\n",
      "(48, 'i')\n",
      "(49, ',')\n",
      "(50, ':')\n",
      "(51, '.')\n",
      "(52, 'z')\n",
      "(53, '<')\n",
      "(54, 'A')\n",
      "(55, 'u')\n",
      "(56, '1')\n",
      "(57, 'L')\n",
      "(58, 'k')\n",
      "(59, 'K')\n",
      "(60, 'P')\n",
      "(61, '>')\n",
      "(62, '&')\n",
      "(63, 'd')\n",
      "(64, 'O')\n",
      "(65, 'T')\n",
      "(66, 'o')\n",
      "(67, 'N')\n",
      "(68, 'F')\n",
      "(69, 'r')\n",
      "(70, \"'\")\n",
      "(71, 'M')\n",
      "(72, 'G')\n",
      "(73, 'R')\n",
      "(74, '?')\n",
      "(75, 'w')\n",
      "(76, 'Q')\n",
      "(77, '7')\n",
      "(78, '(')\n",
      "(79, 'Z')\n",
      "(80, 'v')\n",
      "(81, ' ')\n",
      "(82, 'l')\n",
      "(83, 'y')\n"
     ]
    }
   ],
   "source": [
    "for pair in enumerate(all_characters):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b14cdcc2-9f3b-41fa-bb34-572498ced19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num --> letter\n",
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4573101-c8d8-4a39-8520-db6c8d6e2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# letter --> num\n",
    "encoder = {char: ind for ind, char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21ce0f39-61c8-4423-94ec-19bbb7c77798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{')': 0,\n",
       " '8': 1,\n",
       " 'J': 2,\n",
       " 'V': 3,\n",
       " '}': 4,\n",
       " 'X': 5,\n",
       " 't': 6,\n",
       " '4': 7,\n",
       " 'h': 8,\n",
       " 'x': 9,\n",
       " 'b': 10,\n",
       " 'I': 11,\n",
       " 'Y': 12,\n",
       " '9': 13,\n",
       " '`': 14,\n",
       " 'e': 15,\n",
       " '-': 16,\n",
       " 'E': 17,\n",
       " '[': 18,\n",
       " 'c': 19,\n",
       " ']': 20,\n",
       " 'a': 21,\n",
       " '!': 22,\n",
       " 'S': 23,\n",
       " 'D': 24,\n",
       " '\\n': 25,\n",
       " 'B': 26,\n",
       " 'H': 27,\n",
       " '\"': 28,\n",
       " '|': 29,\n",
       " ';': 30,\n",
       " '5': 31,\n",
       " 'g': 32,\n",
       " 'm': 33,\n",
       " 'U': 34,\n",
       " 'W': 35,\n",
       " '2': 36,\n",
       " '3': 37,\n",
       " 'j': 38,\n",
       " '_': 39,\n",
       " '6': 40,\n",
       " 'n': 41,\n",
       " 's': 42,\n",
       " '0': 43,\n",
       " 'p': 44,\n",
       " 'q': 45,\n",
       " 'f': 46,\n",
       " 'C': 47,\n",
       " 'i': 48,\n",
       " ',': 49,\n",
       " ':': 50,\n",
       " '.': 51,\n",
       " 'z': 52,\n",
       " '<': 53,\n",
       " 'A': 54,\n",
       " 'u': 55,\n",
       " '1': 56,\n",
       " 'L': 57,\n",
       " 'k': 58,\n",
       " 'K': 59,\n",
       " 'P': 60,\n",
       " '>': 61,\n",
       " '&': 62,\n",
       " 'd': 63,\n",
       " 'O': 64,\n",
       " 'T': 65,\n",
       " 'o': 66,\n",
       " 'N': 67,\n",
       " 'F': 68,\n",
       " 'r': 69,\n",
       " \"'\": 70,\n",
       " 'M': 71,\n",
       " 'G': 72,\n",
       " 'R': 73,\n",
       " '?': 74,\n",
       " 'w': 75,\n",
       " 'Q': 76,\n",
       " '7': 77,\n",
       " '(': 78,\n",
       " 'Z': 79,\n",
       " 'v': 80,\n",
       " ' ': 81,\n",
       " 'l': 82,\n",
       " 'y': 83}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04898e3e-3812-49d1-a4b9-699bcc3c4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab5ac5d9-40de-4310-81b7-d52eaf72ae05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81,\n",
       "       81, 81, 81, 81, 81, 56, 25, 81, 81, 68, 69, 66, 33, 81, 46, 21, 48,\n",
       "       69, 15, 42,  6, 81, 19, 69, 15, 21,  6, 55, 69, 15, 42, 81, 75, 15,\n",
       "       81, 63, 15, 42, 48, 69, 15, 81, 48, 41, 19, 69, 15, 21, 42, 15, 49,\n",
       "       25, 81, 81, 65,  8, 21,  6, 81,  6,  8, 15, 69, 15, 10, 83, 81, 10,\n",
       "       15, 21, 55,  6, 83, 70, 42, 81, 69, 66, 42, 15, 81, 33, 48, 32,  8,\n",
       "        6, 81, 41, 15, 80, 15, 69, 81, 63, 48, 15, 49, 25, 81, 81, 26, 55,\n",
       "        6, 81, 21, 42, 81,  6,  8, 15, 81, 69, 48, 44, 15, 69, 81, 42,  8,\n",
       "       66, 55, 82, 63, 81, 10, 83, 81,  6, 48, 33, 15, 81, 63, 15, 19, 15,\n",
       "       21, 42, 15, 49, 25, 81, 81, 27, 48, 42, 81,  6, 15, 41, 63, 15, 69,\n",
       "       81,  8, 15, 48, 69, 81, 33, 48, 32,  8,  6, 81, 10, 15, 21, 69, 81,\n",
       "        8, 48, 42, 81, 33, 15, 33, 66, 69, 83, 50, 25, 81, 81, 26, 55,  6,\n",
       "       81,  6,  8, 66, 55, 81, 19, 66, 41,  6, 69, 21, 19,  6, 15, 63, 81,\n",
       "        6, 66, 81,  6,  8, 48, 41, 15, 81, 66, 75, 41, 81, 10, 69, 48, 32,\n",
       "        8,  6, 81, 15, 83, 15, 42, 49, 25, 81, 81, 68, 15, 15, 63, 70, 42,\n",
       "        6, 81,  6,  8, 83, 81, 82, 48, 32,  8,  6, 70, 42, 81, 46, 82, 21,\n",
       "       33, 15, 81, 75, 48,  6,  8, 81, 42, 15, 82, 46, 16, 42, 55, 10, 42,\n",
       "        6, 21, 41,  6, 48, 21, 82, 81, 46, 55, 15, 82, 49, 25, 81, 81, 71,\n",
       "       21, 58, 48, 41, 32, 81, 21, 81, 46, 21, 33, 48, 41, 15, 81, 75,  8,\n",
       "       15, 69, 15, 81, 21, 10, 55, 41, 63, 21, 41, 19, 15, 81, 82, 48, 15,\n",
       "       42, 49, 25, 81, 81, 65,  8, 83, 81, 42, 15, 82, 46, 81,  6,  8, 83,\n",
       "       81, 46, 66, 15, 49, 81,  6, 66, 81,  6,  8, 83, 81, 42, 75, 15, 15,\n",
       "        6, 81, 42, 15, 82, 46, 81,  6, 66, 66, 81, 19, 69, 55, 15, 82, 50,\n",
       "       25, 81, 81, 65,  8, 66, 55, 81,  6,  8, 21,  6, 81, 21, 69,  6, 81,\n",
       "       41, 66, 75, 81,  6,  8, 15, 81, 75, 66, 69, 82, 63, 70, 42, 81, 46,\n",
       "       69, 15, 42,  8, 81, 66, 69, 41, 21, 33, 15, 41,  6, 49, 25, 81, 81,\n",
       "       54, 41, 63, 81, 66, 41, 82, 83, 81,  8, 15, 69, 21, 82, 63, 81,  6,\n",
       "       66, 81,  6,  8, 15, 81, 32, 21, 55, 63, 83, 81, 42, 44, 69, 48, 41,\n",
       "       32, 49, 25, 81, 81, 35, 48,  6,  8, 48, 41, 81,  6,  8, 48, 41, 15,\n",
       "       81, 66, 75, 41, 81, 10, 55, 63, 81, 10, 55, 69, 48, 15, 42,  6, 81,\n",
       "        6,  8, 83, 81, 19, 66, 41,  6, 15, 41,  6, 49, 25, 81, 81, 54, 41,\n",
       "       63, 81,  6, 15, 41, 63, 15, 69, 81, 19,  8, 55, 69, 82, 81, 33, 21,\n",
       "       58, 70, 42,  6, 81, 75, 21, 42,  6, 15, 81, 48, 41, 81, 41, 48, 32,\n",
       "       32, 21, 69, 63, 48, 41, 32, 50, 25, 81, 81, 81, 81, 60, 48,  6, 83,\n",
       "       81,  6,  8, 15, 81, 75, 66, 69, 82, 63, 49, 81, 66, 69, 81, 15, 82,\n",
       "       42, 15, 81,  6,  8, 48, 42, 81, 32, 82, 55,  6,  6, 66, 41, 81, 10,\n",
       "       15, 49, 25, 81, 81, 81, 81, 65, 66, 81, 15, 21,  6, 81,  6,  8, 15,\n",
       "       81, 75, 66, 69, 82, 63, 70, 42, 81, 63, 55, 15, 49, 81, 10, 83, 81,\n",
       "        6,  8, 15, 81, 32, 69, 21, 80, 15, 81, 21, 41, 63, 81,  6,  8, 15,\n",
       "       15, 51, 25, 25, 25, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81,\n",
       "       81, 81, 81, 81, 81, 81, 81, 81, 81, 36, 25, 81, 81, 35,  8, 15, 41,\n",
       "       81, 46, 66, 69,  6, 83, 81, 75, 48, 41,  6, 15, 69, 42, 81, 42,  8,\n",
       "       21, 82, 82, 81, 10, 15, 42, 48, 15, 32, 15, 81,  6,  8, 83, 81, 10,\n",
       "       69, 66, 75, 49, 25, 81, 81, 54, 41, 63, 81, 63, 48, 32, 81, 63, 15,\n",
       "       15, 44, 81,  6, 69, 15, 41, 19,  8, 15, 42, 81, 48, 41, 81,  6,  8,\n",
       "       83, 81, 10, 15, 21, 55,  6, 83, 70, 42, 81, 46, 48, 15, 82, 63, 49,\n",
       "       25, 81, 81, 65,  8, 83, 81, 83, 66, 55,  6,  8, 70, 42, 81, 44, 69,\n",
       "       66, 55, 63, 81, 82, 48, 80, 15, 69, 83, 81, 42, 66, 81, 32, 21, 52,\n",
       "       15, 63, 81, 66, 41, 81, 41, 66, 75, 49, 25, 81, 81, 35, 48, 82, 82,\n",
       "       81, 10, 15, 81, 21, 81,  6, 21,  6,  6, 15, 69, 15, 63, 81, 75, 15,\n",
       "       15, 63, 81, 66, 46, 81, 42, 33, 21, 82, 82, 81, 75, 66, 69,  6,  8,\n",
       "       81,  8, 15, 82, 63, 50, 81, 81, 25, 81, 81, 65,  8, 15, 41, 81, 10,\n",
       "       15, 48, 41, 32, 81, 21, 42, 58, 15, 63, 49, 81, 75,  8, 15, 69, 15,\n",
       "       81, 21, 82, 82, 81,  6,  8, 83, 81, 10, 15, 21, 55,  6, 83, 81, 82,\n",
       "       48, 15, 42, 49, 25, 81, 81, 35,  8, 15, 69, 15, 81, 21, 82, 82, 81,\n",
       "        6,  8, 15, 81,  6, 69, 15, 21, 42, 55, 69, 15, 81, 66, 46, 81,  6,\n",
       "        8, 83, 81, 82, 55, 42,  6, 83, 81, 63, 21, 83, 42, 30, 25, 81, 81,\n",
       "       65, 66, 81, 42, 21, 83, 81, 75, 48,  6,  8, 48, 41, 81,  6,  8, 48,\n",
       "       41, 15, 81, 66, 75, 41, 81, 63, 15, 15, 44, 81, 42, 55])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b11fc04a-e4f9-4f2a-971a-3e317bd94f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24587d9f-3f37-493d-89a4-3e275df0db92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5927805f-ebe6-44fb-9cff-3b531c20d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "\n",
    "    # encoded_text --> catch of encoder text\n",
    "    # num_uni_chars --> len(set(text))\n",
    "\n",
    "    # create place holder\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "\n",
    "    # need to have float32 to make sure no errors using pytorch later\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8442d0a7-1f64-4951-bb5c-d2aa991fd0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f40554f-c2e6-498c-8da3-6b1decb2fa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e69c1ef0-9cfc-4bbd-aa6e-baea7bfd9de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(arr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e6ea8c1-2d5f-49a5-b62a-997f1325b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37ff1ad1-8925-470c-bb82-e1767fffb24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b8ca71a-8528-4fb8-af0c-74aac8921403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02b99f07-009b-40f3-ac63-38d766d811d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here create a generator, which use yield keyword, not save anything in memory, but generate batches requested\n",
    "\n",
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "\n",
    "    # X: encoded text of length seq_len\n",
    "    # Y: encoded text shifted by one\n",
    "\n",
    "    # the total number of chars used per batch:\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "\n",
    "    # calculate the number of batches available to make, given the length of encoded text\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "\n",
    "    # Cut off the end of the encoded text that won't fit evenly into a batch\n",
    "    # this may cause loss of the some information at the end of text, but as long as your text is long enough, it will not affect your results.\n",
    "    encoded_text = encoded_text[:num_batches_avail*char_per_batch]\n",
    "\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "\n",
    "    # for each row in the array, graph the feature chars and save y as the chars shift by one\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "\n",
    "        # create zeros array to the same shape as x\n",
    "        y = np.zeros_like(x)\n",
    "\n",
    "        try:\n",
    "\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:, n+seq_len]\n",
    "\n",
    "        # at the very end, out of range index error\n",
    "        except:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:,0]\n",
    "\n",
    "        # yield x,y in a tuple\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2b03dbcb-b2ce-4eb4-8df6-e2559b5165b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = encoded_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e73458d1-0096-443e-8ee4-6040370145a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81,\n",
       "       81, 81, 81])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4314c06f-4398-4df8-a1e0-142dfc586f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "164bd893-1899-441e-8067-7bae1ceb1dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text, samp_per_batch=2, seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a7aec85-1b06-4d6b-b68d-06152885356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y =next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b98e6ec-1df2-40a1-900e-1d00b1425d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25, 81, 81, 81, 81],\n",
       "       [81, 81, 81, 81, 81]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf1112d2-bceb-4862-a9be-5e4295b8049b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[81, 81, 81, 81, 81],\n",
       "       [81, 81, 81, 81, 81]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cfee49ca-454b-490e-920b-70093116d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = np.arange(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3bf13f8-abbb-4d1b-9996-9a53ae9e8834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cc677d96-1603-4b0d-b633-0194773205a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text, samp_per_batch=2, seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dfbc5a07-d6d1-40df-9294-4a4c752e5ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e5d2f21-1329-4054-97ad-dfb4b890aaa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [10, 11, 12, 13, 14]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74ada262-9f08-4e55-9d64-3985569a6033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852ee76-b7c0-4586-9683-03871f58374d",
   "metadata": {},
   "source": [
    "## Create LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "02b5271c-7e0d-49dc-b1bb-9f448ffa3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "\n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.5, device='cpu'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.device = device  # set device (either 'cuda' or 'cpu')\n",
    "\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char:ind for ind, char in decoder.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        drop_output = self.dropout(lstm_output)\n",
    "\n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "\n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        return final_out, hidden\n",
    "\n",
    "    def hidden_state(self,batch_size):\n",
    "        # Initialize the hidden state tensors on the correct device\n",
    "        hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).to(self.device),\n",
    "                  torch.zeros(self.num_layers, batch_size, self.num_hidden).to(self.device))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d8248539-124a-48cd-97aa-0c2b21f36e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect device (either 'cuda' if GPU is available or 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9c3c1146-0230-4bb5-b3c2-142cd6cef90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and move it to the approciate device\n",
    "model = CharModel(all_chars=all_characters,\n",
    "                  num_hidden=512,\n",
    "                  num_layers=3,\n",
    "                  drop_prob=0.5,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c6331f84-31cf-4775-b9d1-3030bcff2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df5904c0-7ce9-457a-bd24-d1a856f26772",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca7fbe77-3231-4bb1-86c6-06ffbeadd537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aa7cc4ab-9d9f-4db5-87bf-b58fab37ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "05a6d2c6-0fbd-4374-a35c-6bebbce51ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25c63dc0-25cc-4351-8847-ec4dab1f530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "47c3b5e9-8298-4207-90b6-37b3b656ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c190aed-c9f5-4511-a253-5fce6ce522fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544560"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d280e3cf-0983-4703-9e39-39b417922083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901049"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3c022bc4-85ed-4e8c-b3f0-87fc919e5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "856cced5-3ef5-4e02-a419-a662c69e517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88502e13-69dc-4f0a-9cfc-3ede71163068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "155faa14-88e3-4d54-b810-5b7211caee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073175e4-cdb0-4101-a603-8acb9219e205",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d29bb6fd-4d1c-4f46-be77-74f32dca5d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "epochs = 60\n",
    "batch_size = 100\n",
    "\n",
    "seq_len = 100\n",
    "\n",
    "tracker = 0\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7bb06ba-a9e2-4741-acaa-43ac7b877614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 25 Val Loss: 4.440383434295654\n",
      "Epoch: 0 Step: 50 Val Loss: 4.440383434295654\n",
      "Epoch: 0 Step: 75 Val Loss: 4.440383434295654\n",
      "Epoch: 0 Step: 100 Val Loss: 4.440383434295654\n",
      "Epoch: 0 Step: 125 Val Loss: 4.440383434295654\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m lstm_output, hidden \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(inputs, hidden)\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(lstm_output, targets\u001b[38;5;241m.\u001b[39mview(batch_size\u001b[38;5;241m*\u001b[39mseq_len)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m---> 23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    hidden = model.hidden_state(batch_size)  # Initialize hidden state\n",
    "\n",
    "    for x, y in generate_batches(train_data, batch_size, seq_len):\n",
    "\n",
    "        tracker += 1\n",
    "\n",
    "        # Convert input to one-hot and then to torch tensors\n",
    "        x = one_hot_encoder(x, num_char)\n",
    "\n",
    "        inputs = torch.from_numpy(x).float().to(device)  # move input to device\n",
    "        targets = torch.from_numpy(y).long().to(device)  # move targets to device\n",
    "\n",
    "        # Detach hidden state for the next batch to avoid backpropagating through the entire sequence\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        lstm_output, hidden = model.forward(inputs, hidden)\n",
    "        loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "\n",
    "        # Backward pass and update parameters\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "\n",
    "        if tracker % 25 == 0:\n",
    "\n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()   # Set model to evaluation mode\n",
    "\n",
    "            with torch.no_grad():  # Disable gradient calculation during validation\n",
    "                for x, y in generate_batches(val_data, batch_size, seq_len):\n",
    "    \n",
    "                    x = one_hot_encoder(x, num_char)\n",
    "    \n",
    "                    inputs = torch.from_numpy(x).float().to(device)\n",
    "                    targets = torch.from_numpy(y).long().to(device)\n",
    "            \n",
    "                    val_hidden = tuple([state.data for state in val_hidden])\n",
    "    \n",
    "                    lstm_output, val_hidden = model.forward(inputs, val_hidden)\n",
    "                    val_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
    "    \n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "            model.train()  # Switch back to training mode\n",
    "\n",
    "            print(f'Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "df959230-8fb0-45c4-9a57-d46e32111187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_name = 'hidden512_layers3_shakes.net'\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd111f4e-1ba7-4040-a32c-acad2777552d",
   "metadata": {},
   "source": [
    "## Load saved model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "298ac532-555b-492f-a966-e5d7418165c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters, \n",
    "    num_hidden=512, \n",
    "    num_layers=4, \n",
    "    drop_prob=0.5, \n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eec356c3-6b0f-40a2-b335-fab31e9d7b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(84, 512, num_layers=4, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'example.net'\n",
    "model.load_state_dict(torch.load(model_name, map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f32df362-ea71-4a38-a06b-59dd205ed799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next character\n",
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "\n",
    "    # Encode the input character\n",
    "    encoded_text = model.encoder[char]\n",
    "\n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "\n",
    "    # One-hot encode the input character\n",
    "    encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "\n",
    "    # Convert to torch tensor and move ot the appropriate device\n",
    "    inputs = torch.from_numpy(encoded_text).float().to(model.device)\n",
    "\n",
    "    # detach hidden state to avoid backprop through entire history\n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "\n",
    "    # Perform forward pass through the model\n",
    "    lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "    # Get probabilities of the next character\n",
    "    probs = F.softmax(lstm_out, dim=1).data\n",
    "\n",
    "    # Move probabilities to cpu and process them as numpy arrays\n",
    "    probs = probs.cpu()\n",
    "\n",
    "    probs, index_positions = probs.topk(k)\n",
    "\n",
    "    index_positions = index_positions.numpy().squeeze()\n",
    "\n",
    "    probs = probs.numpy().flatten()\n",
    "\n",
    "    # Normalize the probabilities\n",
    "    probs = probs/probs.sum()\n",
    "\n",
    "    # Randomly choose the next character based on the probabilities\n",
    "    char = np.random.choice(index_positions, p=probs)\n",
    "\n",
    "    # Return the decoded character and the new hidden state\n",
    "    return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a3e24536-b8df-4fe1-8c96-f32071400044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate next char\n",
    "def generate_text(model, size, seed='The', k=1):\n",
    "\n",
    "    model = model.to(model.device)  # Ensure the model is on the correct device\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the output with the seed\n",
    "    output_chars = [c for c in seed]\n",
    "\n",
    "    # Initialize hidden state for batch size of 1\n",
    "    hidden = model. hidden_state(1)\n",
    "\n",
    "    # Prime the model with the seed next\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "\n",
    "    output_chars.append(char)\n",
    "\n",
    "    # Generate text of the specified size\n",
    "    for i in range(size):\n",
    "\n",
    "        char,hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "\n",
    "        output_chars.append(char)\n",
    "\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2bb17423-a3d7-4c5e-b2a0-2aabed9e4022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The VV>Vi>VVV>VV>Vi>>iVVVVV>VVViVV>>>>V>VV>>VVVVVVVV>VVVVi>>>VVV>>>ViVV>VVVVV>iiV>VV>ViVVVVV>V>V>>>>iiVViVViiVViV>VViV>VV>Vi>>V>V>iVV>VVVVViV>V>V>>V>>>VVVVVVVVVVVVVVVVV>VV>VVV>iViVVVVViV>ViV>ViVVVV>VVVVV>VVViVVViVVV>iVVVVVVVVVV>VVVVVV>iVV>VVVVVV>iiVV>VVVVVVV>VVV>V>Vi>>VVVVVV>Vi>VViV>VV>VVVVVVVViiiViVVVV>ViVVi>VVV>VVVV>>iVVViVV>iVV>VVVV>i>i>iVVV>VVVVViiVVi>>VVV>VV>V>VVVVVVVi>ViVVViVVVVVVVi>VV>VVViVViVV>VVViVVVVVVVViViV>>>V>VVV>VVVVVVVV>VVVVV>VV>V>>V>VVii>>>VV>>VVi>VViViV>VVViVViVViV>i>VVVVViiVViVVVVVi>VVi>VVi>VVViVVVVVViViVV>>VVVV>iVVi>VV>>>>>VVViiiV>V>VVVVVVVVVVVVVV>iiVVV>VVVVVV>V>V>VViVVV>iViiVVVV>V>>VVViVVVV>V>VVi>V>>VVVVVVV>VV>iVVVV>VVVVVVViV>VVVVii>VVVVVVVVV>VVViVVVVi>>VVV>VVViV>V>VVVi>>iVVVVVi>VVVVVVVViV>i>iiV>Vi>VVViVVVV>iVVViVVVVVViVVVVV>iiV>VV>>V>VVVVi>V>iVVVVVVV>VVVV>iVV>VV>VVVVViViViVV>>i>VVVVVVVVVV>iVV>VVVVVVV>VVVVVVVVi>i>VVViVVV>VViVVViVViVi>>VViVV>>iVVVi>Vi>iVViVVVV>>V>V>iVVV>>VV>Vii>ViVVV>V>iViViVViVVVVV>>>V>iVVV>ViVVVVVVVVVVVV>VV>>>VVVVVViVVV>VVVVVVViVV>iViVVVViVViViVVVViVV>V\n"
     ]
    }
   ],
   "source": [
    "# Example usage: generate 1000 characters of text\n",
    "print(generate_text(model, 1000, seed='The ', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3baf4a9-7d52-4c80-9154-656a8c1860c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
