{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185983b4-1ab5-4112-aafd-e326448693b2",
   "metadata": {},
   "source": [
    "# Use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c376d7a1-2fd4-4ba6-b4a0-5acb4ca39008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 300 Loss: 0.19923879206180573\n",
      "Epoch: 0 Batch: 600 Loss: 0.3355424106121063\n",
      "Epoch: 0 Batch: 900 Loss: 0.588188111782074\n",
      "Epoch: 0 Batch: 1200 Loss: 0.4802677035331726\n",
      "Epoch: 0 Batch: 1500 Loss: 0.05762867256999016\n",
      "Epoch: 0 Batch: 1800 Loss: 0.04639798775315285\n",
      "Epoch: 0 Batch: 2100 Loss: 0.04718516021966934\n",
      "Epoch: 0 Batch: 2400 Loss: 0.12742990255355835\n",
      "Epoch: 0 Batch: 2700 Loss: 0.008168680593371391\n",
      "Epoch: 0 Batch: 3000 Loss: 0.05294569581747055\n",
      "Epoch: 1 Batch: 300 Loss: 0.23256555199623108\n",
      "Epoch: 1 Batch: 600 Loss: 0.15350341796875\n",
      "Epoch: 1 Batch: 900 Loss: 0.3407380282878876\n",
      "Epoch: 1 Batch: 1200 Loss: 0.052543383091688156\n",
      "Epoch: 1 Batch: 1500 Loss: 0.012765685096383095\n",
      "Epoch: 1 Batch: 1800 Loss: 0.010356596671044827\n",
      "Epoch: 1 Batch: 2100 Loss: 0.009910034015774727\n",
      "Epoch: 1 Batch: 2400 Loss: 0.16558624804019928\n",
      "Epoch: 1 Batch: 2700 Loss: 0.04246605187654495\n",
      "Epoch: 1 Batch: 3000 Loss: 0.0022849454544484615\n",
      "Epoch: 2 Batch: 300 Loss: 0.036422014236450195\n",
      "Epoch: 2 Batch: 600 Loss: 0.016712937504053116\n",
      "Epoch: 2 Batch: 900 Loss: 0.07127358019351959\n",
      "Epoch: 2 Batch: 1200 Loss: 0.011242938227951527\n",
      "Epoch: 2 Batch: 1500 Loss: 0.25782328844070435\n",
      "Epoch: 2 Batch: 1800 Loss: 0.012995941564440727\n",
      "Epoch: 2 Batch: 2100 Loss: 0.07487121224403381\n",
      "Epoch: 2 Batch: 2400 Loss: 0.041749805212020874\n",
      "Epoch: 2 Batch: 2700 Loss: 0.010084045119583607\n",
      "Epoch: 2 Batch: 3000 Loss: 0.11312951892614365\n",
      "Epoch: 3 Batch: 300 Loss: 0.0005206370260566473\n",
      "Epoch: 3 Batch: 600 Loss: 0.061773210763931274\n",
      "Epoch: 3 Batch: 900 Loss: 0.09619767963886261\n",
      "Epoch: 3 Batch: 1200 Loss: 0.03976859152317047\n",
      "Epoch: 3 Batch: 1500 Loss: 0.0023446683771908283\n",
      "Epoch: 3 Batch: 1800 Loss: 0.00113513576798141\n",
      "Epoch: 3 Batch: 2100 Loss: 0.030818650498986244\n",
      "Epoch: 3 Batch: 2400 Loss: 0.014273462817072868\n",
      "Epoch: 3 Batch: 2700 Loss: 0.011918521486222744\n",
      "Epoch: 3 Batch: 3000 Loss: 0.013681747019290924\n",
      "Epoch: 4 Batch: 300 Loss: 0.004380093887448311\n",
      "Epoch: 4 Batch: 600 Loss: 0.0005508458707481623\n",
      "Epoch: 4 Batch: 900 Loss: 0.2929161787033081\n",
      "Epoch: 4 Batch: 1200 Loss: 0.00012778921518474817\n",
      "Epoch: 4 Batch: 1500 Loss: 0.002488679951056838\n",
      "Epoch: 4 Batch: 1800 Loss: 0.0022219265811145306\n",
      "Epoch: 4 Batch: 2100 Loss: 0.04510378837585449\n",
      "Epoch: 4 Batch: 2400 Loss: 0.0015955485869199038\n",
      "Epoch: 4 Batch: 2700 Loss: 0.12885597348213196\n",
      "Epoch: 4 Batch: 3000 Loss: 0.0014052498154342175\n",
      "Duration is 2.2249866724014282 mins\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn. metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.MNIST(root='../Data/', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='../Data/', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=20, shuffle=False)\n",
    "\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 16*5*5)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = F.log_softmax(self.fc3(X), dim=1)\n",
    "\n",
    "        return X\n",
    "\n",
    "model = ConvolutionalNetwork()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# variables (trackers)\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# for loop epochs\n",
    "for i in range(epochs):\n",
    "\n",
    "    train_corr = 0\n",
    "    test_corr = 0\n",
    "\n",
    "    # train\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        b += 1\n",
    "\n",
    "        # X_train = X_train\n",
    "        # y_train = y_train\n",
    "        y_pred = model(X_train)  # no need to flatten for CNN\n",
    "        loss = criterion(y_pred, y_train)\n",
    "\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        train_corr += (predicted == y_train).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b%300 == 0:\n",
    "            print(f'Epoch: {i} Batch: {b} Loss: {loss.item()}')\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(train_corr)\n",
    "    \n",
    "    # test\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "\n",
    "            y_val = model(X_test)\n",
    "            loss = criterion(y_val, y_test)\n",
    "\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            test_corr += (predicted == y_test).sum()\n",
    "\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(test_corr)\n",
    "            \n",
    "current_time = time.time()\n",
    "total_time = current_time - start_time\n",
    "print(f'Duration is {total_time/60} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e84517-a6a9-4358-bf2b-8bd0951267a8",
   "metadata": {},
   "source": [
    "# Try to use GPU efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3656168-bc30-4fc0-8fbc-0be69f1b5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 Loss: 2.278815746307373\n",
      "Epoch: 0 Batch: 300 Loss: 0.6107416152954102\n",
      "Epoch: 0 Batch: 600 Loss: 0.1221124529838562\n",
      "Epoch: 0 Batch: 900 Loss: 0.18486669659614563\n",
      "Epoch: 0 Batch: 1200 Loss: 0.03528972342610359\n",
      "Epoch: 0 Batch: 1500 Loss: 0.1097133532166481\n",
      "Epoch: 0 Batch: 1800 Loss: 0.11011260747909546\n",
      "Epoch: 0 Batch: 2100 Loss: 0.4448215961456299\n",
      "Epoch: 0 Batch: 2400 Loss: 0.1753503978252411\n",
      "Epoch: 0 Batch: 2700 Loss: 0.013160218484699726\n",
      "Epoch: 1 Batch: 0 Loss: 0.0018584579229354858\n",
      "Epoch: 1 Batch: 300 Loss: 0.10041536390781403\n",
      "Epoch: 1 Batch: 600 Loss: 0.21989528834819794\n",
      "Epoch: 1 Batch: 900 Loss: 0.008303202688694\n",
      "Epoch: 1 Batch: 1200 Loss: 0.0019848421216011047\n",
      "Epoch: 1 Batch: 1500 Loss: 0.0009590493282303214\n",
      "Epoch: 1 Batch: 1800 Loss: 0.007465729955583811\n",
      "Epoch: 1 Batch: 2100 Loss: 0.014460556209087372\n",
      "Epoch: 1 Batch: 2400 Loss: 0.046922389417886734\n",
      "Epoch: 1 Batch: 2700 Loss: 0.17186346650123596\n",
      "Epoch: 2 Batch: 0 Loss: 0.04753657802939415\n",
      "Epoch: 2 Batch: 300 Loss: 0.0029641396831721067\n",
      "Epoch: 2 Batch: 600 Loss: 0.014660674147307873\n",
      "Epoch: 2 Batch: 900 Loss: 0.0930716022849083\n",
      "Epoch: 2 Batch: 1200 Loss: 0.04537581279873848\n",
      "Epoch: 2 Batch: 1500 Loss: 0.046804267913103104\n",
      "Epoch: 2 Batch: 1800 Loss: 0.003323640674352646\n",
      "Epoch: 2 Batch: 2100 Loss: 0.17325302958488464\n",
      "Epoch: 2 Batch: 2400 Loss: 0.09373805671930313\n",
      "Epoch: 2 Batch: 2700 Loss: 0.12888740003108978\n",
      "Epoch: 3 Batch: 0 Loss: 0.020017098635435104\n",
      "Epoch: 3 Batch: 300 Loss: 0.1695198118686676\n",
      "Epoch: 3 Batch: 600 Loss: 0.011570966802537441\n",
      "Epoch: 3 Batch: 900 Loss: 0.03863561153411865\n",
      "Epoch: 3 Batch: 1200 Loss: 0.003469036426395178\n",
      "Epoch: 3 Batch: 1500 Loss: 0.21454556286334991\n",
      "Epoch: 3 Batch: 1800 Loss: 0.008897914551198483\n",
      "Epoch: 3 Batch: 2100 Loss: 0.001114198938012123\n",
      "Epoch: 3 Batch: 2400 Loss: 0.004331901203840971\n",
      "Epoch: 3 Batch: 2700 Loss: 0.05724962428212166\n",
      "Epoch: 4 Batch: 0 Loss: 0.021581295877695084\n",
      "Epoch: 4 Batch: 300 Loss: 0.03440672904253006\n",
      "Epoch: 4 Batch: 600 Loss: 0.0001018923576339148\n",
      "Epoch: 4 Batch: 900 Loss: 0.00813149381428957\n",
      "Epoch: 4 Batch: 1200 Loss: 0.00022364182223100215\n",
      "Epoch: 4 Batch: 1500 Loss: 0.0534934476017952\n",
      "Epoch: 4 Batch: 1800 Loss: 0.1358838975429535\n",
      "Epoch: 4 Batch: 2100 Loss: 0.0013244397705420852\n",
      "Epoch: 4 Batch: 2400 Loss: 0.0007644907454960048\n",
      "Epoch: 4 Batch: 2700 Loss: 0.0016263003926724195\n",
      "Duration: 0.85 minutes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")                             # set the device to cuda\n",
    "\n",
    "# Transform to tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load MNIST data\n",
    "train_data = datasets.MNIST(root='../Data/', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='../Data/', train=False, download=True, transform=transform)\n",
    "\n",
    "# Loaders with pin_memory=True for faster data transfer to GPU\n",
    "train_loader = DataLoader(train_data, batch_size=20, shuffle=True, pin_memory=True)                        # set the pin_memory=True\n",
    "test_loader = DataLoader(test_data, batch_size=20, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Define CNN model\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 16*5*5)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = F.log_softmax(self.fc3(X), dim=1)\n",
    "        return X\n",
    "\n",
    "# Initialize model and move to device (GPU or CPU)\n",
    "model = ConvolutionalNetwork().to(device)                                                            # model move to device CUDA\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)                                                       # loss move to device CUDA\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scaler = torch.amp.GradScaler()                                                                    # Set up mixed precision training\n",
    "\n",
    "# Training parameters\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for i in range(epochs):\n",
    "    train_corr = 0\n",
    "    test_corr = 0\n",
    "\n",
    "    # Training phase\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)                                          # X_train and y_train move to device CUDA\n",
    "        \n",
    "        # Forward pass with autocast for mixed precision\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            y_pred = model(X_train)\n",
    "            loss = criterion(y_pred, y_train)\n",
    "\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        train_corr += (predicted == y_train).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass                                                                     # Set up mixed precision training scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if b % 300 == 0:\n",
    "            print(f'Epoch: {i} Batch: {b} Loss: {loss.item()}')\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    train_correct.append(train_corr.item())\n",
    "\n",
    "    # Testing phase (no gradient computation)\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)                                    # X_test and y_test move to device CUDA\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda'):                                            # Set up cuda\n",
    "                y_val = model(X_test)\n",
    "                loss = criterion(y_val, y_test)\n",
    "\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            test_corr += (predicted == y_test).sum()\n",
    "\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(test_corr)\n",
    "\n",
    "current_time = time.time()\n",
    "total_time = current_time - start_time\n",
    "print(f'Duration: {total_time/60:.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f36308-f568-4b52-9674-d766ec35776e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8ff1a3b-4d10-4fc3-ad06-466911b80b0a",
   "metadata": {},
   "source": [
    "## Test other details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "717b8fec-b46e-4f3a-9228-a1dd97bc6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 color channel, 6 filters, 3 by 3 kernal, stride = 1\n",
    "conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "\n",
    "# 6 input filters Conv1, 16 filters (output channels), 3 by 3 kernal, stride = 1\n",
    "conv2 = nn.Conv2d(6, 16, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27ac7dd-8c02-4a83-94d2-3ac7e4ba3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X_train, y_train) in enumerate(train_data):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49f338e-fb52-47ce-b81c-b769d28fe791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e837ec1-ed43-48c2-86df-a4c071109b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124f2183-d1bc-4a8a-ba11-d83909115142",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train.view(1, 1, 28, 28)  # --> 4D batch (batch of 1 image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36fe437e-1da0-4eba-9079-431720c12426",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.relu(conv1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b37c16c-ee34-43a9-84e0-06c1b0756d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 26, 26])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 means 1 single image, 6 is the filter number, 26 = ((28 + 0 (padding) - 3 (kernal size))/stride) + 1\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424b6d56-da83-4489-b78e-da2d2f0b8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.max_pool2d(x, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b8c772-9bdb-4c4d-b2d4-713453d61cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 13, 13])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 26 is height of previous conv1 result, the first 2 is kernal size, the second 2 is the stride size, the 1 is a constant value.\n",
    "# width/height 13 = ((26-2)/2) + 1\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc1090e9-fc86-4dcc-a6a6-c092d33c0c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.relu(conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8a5555e-da9a-4c67-a610-b129dd2c977d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 11, 11])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# width/height 11 = ((13-3)/1) + 1\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "041c7769-cae0-4a3a-b63a-1c1cd2bc601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = F.max_pool2d(x, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a745c5-10a0-4615-9e2f-77ddf6183de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 5, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# width/height 5 = ((11-2)/2) + 1 = 4 + 1\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d55264d4-7c29-46d1-ba22-2d1fbdf4afe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1, 16*5*5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd06bb74-4cdc-44a9-8c84-663c0937632f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b86b428-d092-4247-8bac-b6e4c19a62e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf15076f-0510-4cca-bc8b-995cc9e42f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4060 Laptop GPU'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052477bf-cbc3-4195-9bfb-22d18ae92ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43186322-833c-4c17-9c1c-c069f1df4b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "6\n",
      "864\n",
      "16\n",
      "48000\n",
      "120\n",
      "10080\n",
      "84\n",
      "840\n",
      "10\n",
      "\n",
      "Total number of parameters is 60074\n"
     ]
    }
   ],
   "source": [
    "# 3*3*1*6 (total filter parameters) + 6 (bias) + 3*3*6*16 (total filter parameters) + 16 (bias) \n",
    "# + 400 * 120 (total weight parameters) + 120 (bias) + 120 * 84 (total weight parameters) + 84 (bias) + 84 * 10 (total weight parameters) + 10 (bias)\n",
    "\n",
    "params = 0\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param.numel())\n",
    "    params += param.numel()\n",
    "\n",
    "print(f'\\nTotal number of parameters is {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a04fe7-09e2-4ae1-866d-95c8c7b095eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4ae54-7e15-457c-9e20-6fcac1708bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a7858-94a9-4af6-9ea5-a98dec61b44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
